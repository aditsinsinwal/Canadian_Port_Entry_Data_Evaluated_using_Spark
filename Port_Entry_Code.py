# -*- coding: utf-8 -*-
"""Untitled17.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SrsSCBt_LR_ote5ubYFXe26xNgL6Ylau
"""

!pip install pyspark

from pyspark.sql import SparkSession
from pyspark.sql.functions import month

#Creating spark session
spark = SparkSession.builder \
    .appName("TravelerVolumesAnalysis") \
    .getOrCreate()

#Loading CSV data from Goverment of Canada
data = spark.read.csv("/open-government-traveller-report-daily-en.csv", header=True, inferSchema=True)

#Renaming Column for convenience
data = data.withColumnRenamed("Date", "Travel Date") \
           .withColumnRenamed("Sum of Volume", "Volume")

#Abstracting Month from Travel Date
data = data.withColumn("Month", month("Travel Date"))

#Grouping data by Volume
grouped_data = data.groupBy("Port of Entry", "Month", "Region", "Mode").sum("Volume")

grouped_data.show()

"""Trend Analysis"""

trend_data = grouped_data.groupBy("Month", "Port of Entry").sum("sum(Volume)")
trend_data.show()

"""Comparing Volumes by Different Port of Entry"""

port_entry_volumes = grouped_data.groupBy("Port of Entry").sum("sum(Volume)")
port_entry_volumes.show()

"""Analyzing Regional Variations"""

regional_volumes = grouped_data.groupBy("Region", "Port of Entry").sum("sum(Volume)")
regional_volumes.show()

"""Creating Data Visualization"""

import seaborn as sns
import matplotlib.pyplot as plt

#Converting PySpark datafram to pandas dataframe for data visualization
import pandas
trend_data_pd = pandas.DataFrame(trend_data.toPandas())
port_entry_volumes_pd = port_entry_volumes.toPandas()
regional_volumes_pd = regional_volumes.toPandas()

plt.figure(figsize=(15, 65))
sns.lineplot(data=trend_data_pd, x="Month", y="sum(sum(Volume))", hue="Port of Entry", marker="o")
plt.xlabel("Month")
plt.ylabel("Volume")
plt.title("Trend Analysis of Traveler Volumes by Port of Entry")
plt.legend(title="Port of Entry", bbox_to_anchor=(1.05, 1), loc="upper left")
plt.grid(True)
plt.tight_layout()
plt.show()

plt.figure(figsize=(12, 6))
sns.barplot(data=trend_data_pd, x="Month", y="sum(sum(Volume))", hue="Port of Entry")
plt.xlabel("Month")
plt.ylabel("Volume")
plt.title("Traveler Volumes by Port of Entry and Month")
plt.legend(title="Port of Entry", bbox_to_anchor=(1.05, 1), loc="upper left")
plt.grid(True)
plt.tight_layout()
plt.show()

"""Making the above two plots was probably not a good idea

"""

plt.figure(figsize=(50, 6))
sns.barplot(data=port_entry_volumes_pd, x="Port of Entry", y="sum(sum(Volume))")
plt.xlabel("Port of Entry")
plt.ylabel("Total Volume")
plt.title("Comparing Volumes Between Different Ports of Entry")
plt.xticks(rotation=45, ha="right")
plt.grid(axis="y")
plt.tight_layout()
plt.show()

plt.figure(figsize=(50, 6))
sns.boxplot(data=regional_volumes_pd, x="Port of Entry", y="sum(sum(Volume))", hue="Region")
plt.xlabel("Port of Entry")
plt.ylabel("Volume")
plt.title("Regional Variations in Traveler Volumes by Port of Entry")
plt.legend(title="Region", bbox_to_anchor=(1.05, 1), loc="upper left")
plt.xticks(rotation=45, ha="right")
plt.grid(axis="y")
plt.tight_layout()
plt.show()

#Making Some Predictions using Linear Regression on PySpark
from pyspark.ml.regression import LinearRegression
from pyspark.ml.feature import VectorAssembler

#converting PySpark data frame to Pandas data frame
trend_data_spark = spark.createDataFrame(trend_data_pd)

assembler = VectorAssembler(inputCols=["Month"], outputCol="features")
data = assembler.transform(trend_data_spark).select("features", "sum(sum(Volume))")

#Defining features as Month and target variable as Volume
assembler = VectorAssembler(inputCols=["Month"], outputCol="features")
data = assembler.transform(trend_data_spark).select("features", "sum(sum(Volume))")

#Having a 80 - 20 split (randomly)
train_data, test_data = data.randomSplit([0.8, 0.2], seed=42)

# Converting trend_data_pd to a Spark DataFrame
trend_data_spark = spark.createDataFrame(trend_data_pd)

# Data Cleaning by removing missing values
trend_data_spark_cleaned = trend_data_spark.dropna()

# Define features (Month) and target variable (Volume)
assembler = VectorAssembler(inputCols=["Month"], outputCol="features")
data = assembler.transform(trend_data_spark_cleaned).select("features", "sum(sum(Volume))")

# Split data into training and testing sets (80% training, 20% testing)
train_data, test_data = data.randomSplit([0.8, 0.2], seed=42)

# Build and train a linear regression model
lr = LinearRegression(featuresCol="features", labelCol="sum(sum(Volume))")
lr_model = lr.fit(train_data)

#Build and train a linear regression model
lr = LinearRegression(featuresCol="features", labelCol="sum(sum(Volume))")
lr_model = lr.fit(train_data)

# Make predictions on the test data
predictions = lr_model.transform(test_data)

# Show predictions
predictions.select("features", "sum(sum(Volume))", "prediction").show()

#Evaluating the above prediction
from pyspark.ml.evaluation import RegressionEvaluator
evaluator = RegressionEvaluator(labelCol="sum(sum(Volume))", predictionCol="prediction", metricName="rmse")
rmse = evaluator.evaluate(predictions)

print("Root Mean Squared Error (RMSE) on test data:", rmse)

"""The above RMSE value is much much much much higher than expected so we would need a better model"""